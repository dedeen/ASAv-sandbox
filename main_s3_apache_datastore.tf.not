# Terraform script to build an S3 bucket and store configuration files for the Apache webservers in this environment. 

#  Creating an S3 bucket for files to be retrieved by instances
resource "aws_s3_bucket" "webserver-s3-ds" {
  bucket = "webserver-s3-ds"
  
    tags = {
    Name = "webserver-s3-ds"
    Owner = "dan-via-terraform"
  }
}

resource "aws_s3_bucket_acl" "acl-websrv-s3-ds" {
  bucket = aws_s3_bucket.webserver-s3-ds.id
  acl    = "private"
 }

# Set up directories for webserver configuration files:
#    /HTML-80   -> html files for the HTTP servers serving on port 80
#    /HTML-443  -> html files for the HTTPS servers serving on port 443
#    /scripts   -> script repo for multiple purposes 
#    /reserved  -> spare subdirectory for future use without making big changes in scripts 

resource "aws_s3_object" "slash_80" {
  bucket                    = aws_s3_bucket.webserver-s3-ds.id
  key                       = "/HTML-80/"
  content                   = "application/x-directory"
}

resource "aws_s3_object" "slash_443" {
  bucket                    = aws_s3_bucket.webserver-s3-ds.id
  key                       = "/HTML-443/"
  content                   = "application/x-directory"
}

resource "aws_s3_object" "slash_scripts" {
  bucket                    = aws_s3_bucket.webserver-s3-ds.id
  key                       = "/scripts/"
  content                   = "application/x-directory"
}

resource "aws_s3_object" "slash_reserved" {
  bucket                    = aws_s3_bucket.webserver-s3-ds.id
  key                       = "/reserved/"
  content                   = "application/x-directory"
}

# Copy the files from local (pulled down when repo cloned), to the s3 bucket and directories created above
resource "aws_s3_object" "index80_html" {
  bucket                   = aws_s3_bucket.pavm-s3-ds.id
  key                      = "index.html"
  source                   = "./Webservers/HTML-80/index80.html"
  force_destroy            = true
}
  

# Now that the directories and files are in the s3 bucket, we need to allow the EC2 webservers access the bucket to copy files over. 
#  IAM policy & role for the EC2 instances to access files on the datastore 
data "aws_iam_policy_document" "ec2_assume_role" { 
  statement {
     actions = ["sts:AssumeRole"]
    
     principals {
       type        = "Service"
       identifiers = ["ec2.amazonaws.com"]
     }
   }
 }

# IAM Role associated with the policy document created above 
 resource "aws_iam_role" "ec2_iam_role" {
   name                = "ec2-iam-role"
   path                = "/"
   assume_role_policy  = data.aws_iam_policy_document.ec2_assume_role.json
 }

# EC2 instance profile 
resource "aws_iam_instance_profile" "ec2_profile" {
   name = "ec2-profile"
   role = aws_iam_role.ec2_iam_role.name
 }

# Policy attachments (2) 
resource "aws_iam_policy_attachment" "ec2_attach1" {
  name       = "ec2-iam-attachment"
  roles      = [aws_iam_role.ec2_iam_role.id]
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

resource "aws_iam_policy_attachment" "ec2_attach2" {
  name       = "ec2-iam-attachment"
  roles      = [aws_iam_role.ec2_iam_role.id]
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM"
}

# S3 policy - allows all operations --> should tighten this for production use 
resource "aws_iam_policy" "s3-ec2-policy" {
  name        = "s3-ec2-policy"
  description = "S3 ec2 policy"
  
  policy = jsonencode({  
    Version = "2012-10-17"
    Statement = [
      {
        Action = [
          "s3:*"
        ]
        Effect   = "Allow"
        Resource = "*"
      },
    ]
  })
}

# Attach S3 Policies to Instance Role
resource "aws_iam_policy_attachment" "s3_attach" {
  name       = "s3-iam-attachment"
  roles      = [aws_iam_role.ec2_iam_role.id]
  policy_arn = aws_iam_policy.s3-ec2-policy.arn
}
##

##
